# AI

## AI 模型安全（AI Model Security）开源项目

### 对抗攻击 / 鲁棒性 / 防御
- **Adversarial Robustness Toolbox (ART)**  
  https://github.com/IBM/adversarial-robustness-toolbox  
  > 对抗攻击与防御的综合框架，支持 PyTorch / TensorFlow / sklearn

- **secml**  
  https://github.com/pralab/secml  
  > 安全机器学习库，支持对抗样本、数据中毒、模型鲁棒性评估

- **AdvBox**  
  https://github.com/advboxes/AdvBox  
  > 多框架对抗样本生成工具，适合攻防测试与教学

- **GitHub Topic: adversarial-defenses**  
  https://github.com/topics/adversarial-defenses  
  > GitHub 官方主题页，聚合大量对抗防御项目

---

### LLM / Prompt 安全 / 红队测试
- **CAI – Adversarial ML Framework**  
  https://github.com/aliasrobotics/cai  
  > 自动化对抗测试、红队流程、模型鲁棒性评估

---

### 教学 / 实验 / 攻防演示
- **AI Security Training Lab**  
  https://github.com/citizenjosh/ai-security-training-lab  
  > AI 安全攻防实验环境，覆盖 Prompt 注入、模型抽取等

---

### 资源汇总 / 研究资料
- **Awesome-LLM4Security**  
  https://github.com/liu673/Awesome-LLM4Security  
  > LLM 安全研究资源合集（工具、论文、数据集）